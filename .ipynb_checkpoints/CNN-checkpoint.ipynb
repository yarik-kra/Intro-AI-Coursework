{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN - TRAFFIC SIGNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES AND READING THE CSV FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ClassId                          Name\n",
      "0         0           Speed limit (5km/h)\n",
      "1         1          Speed limit (15km/h)\n",
      "2         2          Speed limit (30km/h)\n",
      "3         3          Speed limit (40km/h)\n",
      "4         4          Speed limit (50km/h)\n",
      "5         5          Speed limit (60km/h)\n",
      "6         6          Speed limit (70km/h)\n",
      "7         7          speed limit (80km/h)\n",
      "8         8      Dont Go straight or left\n",
      "9         9     Dont Go straight or Right\n",
      "10       10              Dont Go straight\n",
      "11       11                  Dont Go Left\n",
      "12       12         Dont Go Left or Right\n",
      "13       13                 Dont Go Right\n",
      "14       14       Dont overtake from Left\n",
      "15       15                      No Uturn\n",
      "16       16                        No Car\n",
      "17       17                       No horn\n",
      "18       18          Speed limit (40km/h)\n",
      "19       19          Speed limit (50km/h)\n",
      "20       20          Go straight or right\n",
      "21       21                   Go straight\n",
      "22       22                       Go Left\n",
      "23       23              Go Left or right\n",
      "24       24                      Go Right\n",
      "25       25                     keep Left\n",
      "26       26                    keep Right\n",
      "27       27          Roundabout mandatory\n",
      "28       28            watch out for cars\n",
      "29       29                          Horn\n",
      "30       30             Bicycles crossing\n",
      "31       31                         Uturn\n",
      "32       32                  Road Divider\n",
      "33       33               Traffic signals\n",
      "34       34                  Danger Ahead\n",
      "35       35                Zebra Crossing\n",
      "36       36             Bicycles crossing\n",
      "37       37             Children crossing\n",
      "38       38   Dangerous curve to the left\n",
      "39       39  Dangerous curve to the right\n",
      "40       40                      Unknown1\n",
      "41       41                      Unknown2\n",
      "42       42                      Unknown3\n",
      "43       43          Go right or straight\n",
      "44       44           Go left or straight\n",
      "45       45                      Unknown4\n",
      "46       46                  ZigZag Curve\n",
      "47       47                Train Crossing\n",
      "48       48            Under Construction\n",
      "49       49                      Unknown5\n",
      "50       50                        Fences\n",
      "51       51       Heavy Vehicle Accidents\n",
      "52       52                      Unknown6\n",
      "53       53                      Give Way\n",
      "54       54                   No stopping\n",
      "55       55                      No entry\n",
      "56       56                      Unknown7\n",
      "57       57                      Unknown8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "\n",
    "file_path = \"/Users/ibhaankudalkar/Downloads/archive/labels.csv\"\n",
    "\n",
    "filename_read = os.path.join(file_path, \"/Users/ibhaankudalkar/Downloads/archive/labels.csv\")\n",
    "df = pd.read_csv(filename_read)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- IMPORTING PYTORCH LIBRARY\n",
    "- USING TRANSFORM TO PREPROCESS EACH IMAGE\n",
    "- RESIZING 32X32 FOR FIXED PIXEL SIZE\n",
    "- TENSOR TO CONVERT IN PYTORCH METHOD \n",
    "- NORMALIZE FOR STANDARD RANGE\n",
    "- LOAD THE IMAGE DATA\n",
    "- SPLIT THE DATA AND PREPARE FOR TRAINING \n",
    "-  FIRST THREE SAMPLE IMAGES OF OUR DATA IS VIEWED ALONG WITH THEIR CLASS NAMES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.90588236..0.6784314].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.6156863..0.45882356].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.7019608..0.7254902].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 58\n",
      "Train size: 2919\n",
      "Validation size: 625\n",
      "Test size: 626\n",
      "4170\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# CNN Implementation in pytorch \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import torch.optim \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# transformation \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='/Users/ibhaankudalkar/Downloads/archive/traffic_Data/DATA', transform=transform)\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# train test split\n",
    "train_size = int(0.7 * len(dataset)) # 70\n",
    "val_size = int(0.15 * len(dataset)) # 15\n",
    "test_size = len(dataset) - train_size - val_size # 15\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(\"Train size:\", train_size)\n",
    "print(\"Validation size:\", val_size)\n",
    "print(\"Test size:\", test_size)\n",
    "print(train_size + val_size + test_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Sample of images \n",
    "def samples(images, labels, class_names):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(len(images)):  \n",
    "        plt.subplot(1, len(images), i + 1)\n",
    "        plt.imshow(images[i].permute(1, 2, 0).numpy())  \n",
    "        plt.title(f\"Class: {class_names[labels[i]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# batch of data\n",
    "data_iter = iter(train_loader)\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# first 3 images\n",
    "samples(images[:3], labels[:3], dataset.classes)\n",
    "\n",
    "# print(f\"Image shape: {sample_images[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A CNN CLASS IS DEFINED WITH THREE CONVOLUTIONAL LAYERS \n",
    "- MAX POOLING WITH A 2X2 FILTER REDUCING THE SPATIAL DIMENSION BY HALF\n",
    "- A DENSE LAYER IS USED TO ADJUST THE ACTUAL IMAGE SIZE\n",
    "- A DROPOUT LAYER WAS ADDED TO MINIMIZE OVERFITTING \n",
    "\n",
    "- A FORWARD PASS IS APPLIED TO INPUT X \n",
    "- RELU ACTIVATION FUNCTION ON THE THREE CONVOLUTIONAL LAYERS OF INPUT X WITH MAX POOLING \n",
    "- X.VIEW FLATTENS THE TENSOR TO 1D VECTOR\n",
    "- RELU IS APPLIED TO INPUT X TO FLATTEN THE VECTOR AND TO PRODUCE THE LOGITS FOR CLASSIFICATION\n",
    "- YOU THEN RETURN X\n",
    "\n",
    "- INITIALIZATION\n",
    "- CREATE A INSTANCE OF CNN MODEL\n",
    "- DEFINE THE LOSS FUNCTION \n",
    "- ADAM OPTIMIZER IS USED WITH PARAMETERS AND LEARNING RATE 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN MODEL CLASS\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.c1 = nn.Linear(128 * 4 * 4, 512)  \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.c2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4) \n",
    "        x = F.relu(self.c1(x))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        x = self.c2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = CNN(num_classes=num_classes)\n",
    "criterion = nn.Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FUNCTION TRAIN MODEL IS USED TO EVALUATE PERFORMANCE OF EACH EPOCH\n",
    "- LOOPING THE NUMBER OF EPOCHS TO TRAIN WITH A SMALL BATCH IN THE TRAINING SET \n",
    "\n",
    "- VALIDATION LOOP IS USED TO VALIDATE THE LOOPS WITHOUT UPDATING THE WEIGHTS\n",
    "\n",
    "- PRINT FOR METRICS AFTER EACH EPOCH\n",
    "\n",
    "- LASTLY CALLS THE TRAIN MODEL FOR 8 EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 0.1548, Validation Loss: 0.0908, Validation Accuracy: 97.76%\n",
      "Epoch 2/8, Train Loss: 0.0377, Validation Loss: 0.0383, Validation Accuracy: 98.88%\n",
      "Epoch 3/8, Train Loss: 0.0099, Validation Loss: 0.0156, Validation Accuracy: 99.36%\n",
      "Epoch 4/8, Train Loss: 0.0054, Validation Loss: 0.0100, Validation Accuracy: 99.84%\n",
      "Epoch 5/8, Train Loss: 0.0009, Validation Loss: 0.0053, Validation Accuracy: 99.84%\n",
      "Epoch 6/8, Train Loss: 0.0007, Validation Loss: 0.0078, Validation Accuracy: 99.68%\n",
      "Epoch 7/8, Train Loss: 0.0011, Validation Loss: 0.0111, Validation Accuracy: 99.68%\n",
      "Epoch 8/8, Train Loss: 0.0004, Validation Loss: 0.0042, Validation Accuracy: 99.84%\n"
     ]
    }
   ],
   "source": [
    "# train model for each epoch\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=8):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, \"\n",
    "              f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "              f\"Validation Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "              f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FUNCTION TEST MODEL IN EVALUATION ITERATES THROUGH THE TEST DATA \n",
    "- PRINTS THE ACCURACY SCORE \n",
    "- LASTLY, CALLS THE FUNCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 99.84%\n"
     ]
    }
   ],
   "source": [
    "# Testing and Evaluation\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Accuracy Score: {100 * correct / total:.2f}%\")\n",
    "\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SAVE THE TRAINED MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model, 'model.pth')\n",
    "print('Saved trained model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
